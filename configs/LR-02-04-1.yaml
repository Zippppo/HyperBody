# Default training configuration
# Copy this file and modify for different experiments
# 这个版本下，ribs的倍率放大两倍，class 0 

# Data
data_dir: "Dataset/voxel_data"
split_file: "Dataset/dataset_split.json"
tree_file: "Dataset/tree.json"
dataset_info_file: "Dataset/dataset_info.json"
num_classes: 70
voxel_size: 4.0
volume_size: [144, 128, 268]  # X, Y, Z

# Model
in_channels: 1
base_channels: 32
num_levels: 4

# Dense Bottleneck
growth_rate: 32
dense_layers: 4
bn_size: 4

# Hyperbolic
hyp_embed_dim: 32
hyp_curv: 1.0
hyp_weight: 0.08                # Moderate increase (original 0.05, aggressive 0.1)
hyp_margin: 0.15                # Moderate margin (original 0.1, aggressive 0.25)
hyp_samples_per_class: 96       # More samples for small classes like ribs
hyp_num_negatives: 24           # Key: 24 negatives = all ribs can contrast each other
hyp_min_radius: 0.1
hyp_max_radius: 3.0             # Moderate expansion (original 2.0, aggressive 4.0)
hyp_direction_mode: "random"    # "random" or "semantic"
hyp_text_embedding_path: "Dataset/text_embeddings/sat_label_embeddings.pt"
hyp_freeze_epochs: 15           # Moderate freeze (original 10, aggressive 20)
hyp_text_lr_ratio: 0.01  # Text embedding LR = base_lr * ratio
hyp_text_grad_clip: 0.1  # Gradient clip for text embeddings (first unfreeze epoch)

# Training
batch_size: 1
num_workers: 0
epochs: 120
lr: 0.001
weight_decay: 0.00001
grad_clip: 1.0

# AMP
use_amp: true

# Loss
ce_weight: 0.5
dice_weight: 0.5

# LR scheduler
lr_patience: 10
lr_factor: 0.5

# Checkpoint
checkpoint_dir: "checkpoints/LR-02-04-1"
save_every: 5
log_dir: "runs/LR-02-04-1"

# GPU
gpu_ids: [0, 1]

# Resume (empty string means start fresh)
resume: ""

# Embedding tracking (visualize label embedding evolution)
track_embeddings: true
