# Training configuration for graph distance-based negative sampling
# Uses tree distance + spatial adjacency for curriculum hard-negative mining

# Data
data_dir: "Dataset/voxel_data"
split_file: "Dataset/dataset_split.json"
tree_file: "Dataset/tree.json"
dataset_info_file: "Dataset/dataset_info.json"
num_classes: 70
voxel_size: 4.0
volume_size: [144, 128, 268]  # X, Y, Z

# Model
in_channels: 1
base_channels: 32
num_levels: 4

# Dense Bottleneck
growth_rate: 32
dense_layers: 4
bn_size: 4

# Hyperbolic
hyp_embed_dim: 32
hyp_curv: 1.0
hyp_weight: 0.05
hyp_margin: 0.1
hyp_samples_per_class: 64
hyp_num_negatives: 8
# Curriculum Negative Mining
hyp_t_start: 2.0          # Initial temperature (high = random sampling)
hyp_t_end: 0.1            # Final temperature (low = hard negative mining)
hyp_warmup_epochs: 5      # Pure random sampling for first N epochs
hyp_min_radius: 0.1
hyp_max_radius: 2.0
hyp_direction_mode: "random"  # "random" or "semantic"
hyp_text_embedding_path: "Dataset/text_embeddings/sat_label_embeddings.pt"
hyp_freeze_epochs: 10  # Freeze label embeddings for first N epochs
hyp_text_lr_ratio: 0.01  # Text embedding LR = base_lr * ratio
hyp_text_grad_clip: 0.1  # Gradient clip for text embeddings (first unfreeze epoch)
hyp_distance_mode: "graph"  # Use graph distance (tree + spatial adjacency)
spatial_dilation_radius: 3  # Cube dilation radius
spatial_lambda: 1.0  # Spatial edge scale factor
spatial_epsilon: 0.01  # Prevents division by zero
spatial_contact_matrix: "checkpoints/LR-Curriculum-GraphDistance/contact_matrix.pt"  # Deprecated for graph mode; kept for compatibility
graph_distance_matrix: "Dataset/graph_distance_matrix.pt"

# Training
batch_size: 2
num_workers: 0
epochs: 120
lr: 0.001
weight_decay: 0.00001
grad_clip: 1.0

# AMP
use_amp: true

# Loss
ce_weight: 0.5
dice_weight: 0.5

# LR scheduler
lr_patience: 10
lr_factor: 0.5

# Checkpoint
checkpoint_dir: "checkpoints/LR-Curriculum-GraphDistance"
save_every: 1
log_dir: "runs/LR-Curriculum-GraphDistance"

# GPU
gpu_ids: [0, 1]

# Resume (empty string means start fresh)
resume: ""

# Embedding tracking (visualize label embedding evolution)
track_embeddings: true
